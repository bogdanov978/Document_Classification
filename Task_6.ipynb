{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тренировочное множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list(open('data/news/news_train.txt', 'r', encoding='utf-8'))\n",
    "#lines = lines2[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_text(text, morph):\n",
    "    result = []\n",
    "    for word in text:\n",
    "        result.append(morph.parse(word)[0].normal_form)\n",
    "    return result\n",
    "\n",
    "def prepare_data(lines): \n",
    "    # токенизация\n",
    "    tokenized_lines = []\n",
    "    for line in lines:\n",
    "        label, head, text = line.split('\\t')\n",
    "        head = re.findall(r'\\w+', head.lower())\n",
    "        text = re.findall(r'\\w+', text.lower())\n",
    "        tokenized_lines.append({'label': label, 'head': head, 'text': text})\n",
    "        \n",
    "    #приведение к нормальной форме\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    for line in tokenized_lines:\n",
    "        line['head'] = normalized_text(line['head'], morph)\n",
    "        line['text'] = normalized_text(line['text'], morph)\n",
    "    \n",
    "    return tokenized_lines\n",
    "\n",
    "train = prepare_data(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train)\n",
    "w2v_dim = 150\n",
    "sentences = train_df['text'].append(train_df['head'])\n",
    "word2vec = gensim.models.Word2Vec(sentences, size = w2v_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('малкин', 0.918655514717102), ('испанец', 0.9180492758750916), ('голкипер', 0.9122662544250488), ('кириленко', 0.906489372253418), ('питтсбург', 0.8967603445053101), ('аргентинец', 0.894297182559967), ('надаль', 0.8928858041763306), ('пингвинс', 0.8928146958351135), ('отличиться', 0.8912882804870605), ('дацюк', 0.8901466131210327)] \n",
      "\n",
      "[('институт', 0.7764558792114258), ('колледж', 0.7548238039016724), ('мгу', 0.7534153461456299), ('политехнический', 0.7458568811416626), ('факультет', 0.7162067294120789), ('гарвардский', 0.7026298642158508), ('калифорнийский', 0.7022683024406433), ('мичиганский', 0.6985563039779663), ('профессор', 0.6860417127609253), ('стэнфордский', 0.6814392805099487)] \n",
      "\n",
      "[('apple', 0.857429027557373), ('lg', 0.8460652828216553), ('htc', 0.8358925580978394), ('смартфон', 0.8341237902641296), ('motorola', 0.8210699558258057), ('планшет', 0.8181561231613159), ('sony', 0.8151872754096985), ('nokia', 0.8067356944084167), ('ipad', 0.8054453134536743), ('microsoft', 0.8039896488189697)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(word2vec.wv.most_similar(positive=['овечкин']), '\\n')\n",
    "print(word2vec.wv.most_similar(positive=['университет']), '\\n')\n",
    "print(word2vec.wv.most_similar(positive=['samsung']), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def document_vectorization(X, word2vec, w2v_dim):\n",
    "    tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "    tfidf.fit(X)\n",
    "    \n",
    "    tf_weights = { word : tfidf.idf_[ind] for word, ind in tfidf.vocabulary_.items() } # пары {слово : tf-idf }\n",
    "    \n",
    "    result = np.array([np.mean([word2vec[w] * tf_weights[w] for w in words if w in word2vec] or [np.zeros(w2v_dim)], axis=0) for words in X])    \n",
    "    return result\n",
    "\n",
    "def set_labels(Y):\n",
    "    labels = {\n",
    "                'science': 0,\n",
    "                'style': 1,\n",
    "                'culture': 2,\n",
    "                'life': 3,\n",
    "                'economics': 4,\n",
    "                'business': 5,\n",
    "                'travel': 6,\n",
    "                'forces': 7,\n",
    "                'media': 8,\n",
    "                'sport': 9\n",
    "               }\n",
    "    return Y.map(lambda label: labels[label]).to_list()\n",
    "    \n",
    "x_train = document_vectorization(train_df['text'], word2vec, w2v_dim)\n",
    "y_train = set_labels(train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_test = list(open('data/news/news_test.txt', 'r', encoding='utf-8'))\n",
    "test = prepare_data(lines_test)\n",
    "test_df = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "x_test = document_vectorization(test_df['text'], word2vec, w2v_dim)\n",
    "y_test = set_labels(test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8503333333333334"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.726"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
